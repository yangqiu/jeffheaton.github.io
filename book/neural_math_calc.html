<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Extra Material for Introduction to the Math of Neural Networks | Heaton Research</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Back Propagation (backprop) is one of the oldest learning methods for neural networks.  Back Propagation is a type of propagation training. Weight Update CalculationLayer deltas must be calculated wor">
<meta property="og:type" content="website">
<meta property="og:title" content="Extra Material for Introduction to the Math of Neural Networks">
<meta property="og:url" content="http://www.heatonresearch.com/book/neural_math_calc.html">
<meta property="og:site_name" content="Heaton Research">
<meta property="og:description" content="Back Propagation (backprop) is one of the oldest learning methods for neural networks.  Back Propagation is a type of propagation training. Weight Update CalculationLayer deltas must be calculated wor">
<meta property="og:updated_time" content="2017-07-01T03:28:39.122Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Extra Material for Introduction to the Math of Neural Networks">
<meta name="twitter:description" content="Back Propagation (backprop) is one of the oldest learning methods for neural networks.  Back Propagation is a type of propagation training. Weight Update CalculationLayer deltas must be calculated wor">
<meta name="twitter:creator" content="@jeffheaton">
  
    <link rel="alternate" href="/atom.xml" title="Heaton Research" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
  

  

  <link rel="stylesheet" href="/css/styles.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-5393865-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics --><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
        <a class="navbar-brand" href="/">Heaton Research</a>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/about/">About</a></li>
        
          <li><a class=""
                 href="/jeff/">Blog</a></li>
        
          <li><a class=""
                 href="/contact.html">Contact</a></li>
        
          <li><a class=""
                 href="/encog/">Encog</a></li>
        
          <li><a class=""
                 href="/book/">Books</a></li>
        
          <li><a class=""
                 href="/aifh/">AIFH</a></li>
        
          <li><a class=""
                 href="/jeff_index/">Articles</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <h1>Extra Material for Introduction to the Math of Neural Networks</h1>
<div class="row">
    <div class="col-sm-12 blog-main">
      <article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">


  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Back Propagation (backprop) is one of the oldest learning methods for neural networks.  Back Propagation is a type of propagation training.</p>
<h1 id="Weight-Update-Calculation"><a href="#Weight-Update-Calculation" class="headerlink" title="Weight Update Calculation"></a>Weight Update Calculation</h1><p>Layer deltas must be calculated working backwards through the network.  </p>
<p>Assuming a linear error function, such as the following.</p>
<p>$$ E = (a-i) $$</p>
<p>The layer deltas are calculated with the following formula.</p>
<p>$$ \delta_i = \begin{cases}-E f’_i &amp; \mbox{, output nodes}\<br>f’_i \sum<em>k w</em>{ki}\delta_k &amp; \mbox{, input/hidden nodes}\<br>\end{cases} $$</p>
<p>The above equation has two modes, depending on if you are calculating for hidden or interior (input/hidden) nodes.  For output nodes take the inverse of the error multiplied by the derivative of the sum (pre-activation function) of the node. For interior nodes multiply by the derivative of the sum (pre-activation function) of the node by the sum of each weight multiplied by the node delta from the next layer(that was just calculated in the previous step).</p>
<p>The node deltas can be used to find the partial derivatives for the weights, which are the gradients.</p>
<p>$$ \frac{ \partial E}{\partial w_{(ik)}} = \delta_k \cdot o_i $$</p>
<p>The above equation should be used once for each weight in the network.  It calculates the gradient from weight ‘’’k’’’ to weight ‘’’i’’’.  Always forward from ‘’’k’’’ to ‘’’i’’’.  For example, ‘’’k’’’ might be in the hidden layer and ‘’’i’’’ in the output.  The above equation multiplies the node delta of ‘’’k’’’ by the output(post activation function) of ‘’’i’’’.</p>
<p>The actual weights are updated using the equation below.</p>
<p>$$ \Delta{w<em>{(t)}} = \epsilon \frac{ \partial E}{\partial w</em>{(t)}} + \alpha \Delta{w_{(t-1)}}  $$<br>Where ‘’’E’’’ is the output of the [[Error Function]].</p>
<h1 id="Example-Calculation"><a href="#Example-Calculation" class="headerlink" title="Example Calculation"></a>Example Calculation</h1><p>To establish some baseline numbers for backpropagation calculation I am including this section.  I will also use this section to test future versions of Encog backprop to make sure that nothing fundamental changes.  If you are using standard backpropagation, and start with the same weights, you should end up with the same numbers as me.  The weights I used are summarized here.  There is nothing special about this weight set, they are random.  It is just a common starting point.</p>
<pre>Weight 0: H1->O1, -0.22791948943117624
Weight 1: H2->O1, 0.581714099641357
Weight 2: B2->O1, 0.7792991203673414
Weight 3: I1->H1, -0.06782947598673161
Weight 4: I2->H1, 0.22341077197888182
Weight 5: B1->H1, -0.4635107399577998
Weight 6: I1->H2, 0.9487814395569221
Weight 7: I2->H2, 0.461587116462548
Weight 8: B1->H2, 0.09750161997450091</pre>

<p>This neural network uses [[bias]] and has two input neurons, two hidden neurons and a single output neuron.  The code is shown here, in Java.  C# should produce exactly the same numbers.</p>
<pre>package org.encog.examples.neural.xor;

import org.encog.Encog;
import org.encog.engine.network.activation.ActivationSigmoid;
import org.encog.mathutil.randomize.ConsistentRandomizer;
import org.encog.ml.data.MLData;
import org.encog.ml.data.MLDataPair;
import org.encog.ml.data.MLDataSet;
import org.encog.ml.data.basic.BasicMLDataSet;
import org.encog.neural.networks.BasicNetwork;
import org.encog.neural.networks.layers.BasicLayer;
import org.encog.neural.networks.training.propagation.back.Backpropagation;

public class XORConst {

    /**
     * The input necessary for XOR.
     */
    public static double XOR_INPUT[][] = { { 1.0, 0.0 }, { 0.0, 0.0 },
            { 0.0, 1.0 }, { 1.0, 1.0 } };

    /**
     * The ideal data necessary for XOR.
     */
    public static double XOR_IDEAL[][] = { { 1.0 }, { 0.0 }, { 1.0 }, { 0.0 } };

    /**
     * The main method.
     * @param args No arguments are used.
     */
    public static void main(final String args[]) {

        // create a neural network, without using a factory
        BasicNetwork network = new BasicNetwork();
        network.addLayer(new BasicLayer(null,true,2));
        network.addLayer(new BasicLayer(new ActivationSigmoid(),true,2));
        network.addLayer(new BasicLayer(new ActivationSigmoid(),false,1));
        network.getStructure().finalizeStructure();
        network.reset();
        new ConsistentRandomizer(-1,1,500).randomize(network);
        System.out.println(network.dumpWeights());

        // create training data
        MLDataSet trainingSet = new BasicMLDataSet(XOR_INPUT, XOR_IDEAL);

        // train the neural network
        final Backpropagation train = new Backpropagation(network, trainingSet, 0.7, 0.3);
        train.fixFlatSpot(false);

        int epoch = 1;

        do {
            train.iteration();
            System.out
                    .println("Epoch #" + epoch + " Error:" + train.getError());
            epoch++;
        } while(train.getError() > 0.01);

        // test the neural network
        System.out.println("Neural Network Results:");
        for(MLDataPair pair: trainingSet ) {
            final MLData output = network.compute(pair.getInput());
            System.out.println(pair.getInput().getData(0) + "," + pair.getInput().getData(1)
                    + ", actual=" + output.getData(0) + ",ideal=" + pair.getIdeal().getData(0));
        }

        Encog.getInstance().shutdown();
    }
}
</pre>

<h1 id="Iteration-1"><a href="#Iteration-1" class="headerlink" title="Iteration 1"></a>Iteration 1</h1><h2 id="Iteration-1-Neural-Network-Output"><a href="#Iteration-1-Neural-Network-Output" class="headerlink" title="Iteration 1:Neural Network Output"></a>Iteration 1:Neural Network Output</h2><p>You can see the entire neural network here.</p>
<p>[[Image:Backprop-1.png]]</p>
<p>This shows the weights and neural network outputs for each location.  The above chart shows the neural network output for the input of [1,0].  The weights are the same as were mentioned in the previous section.  No updates to the weights have been done at this point.<br>====Iteration 1:Calculate Gradients====<br>‘’’Training Element #1’’’</p>
<pre>Input: [1.0, 0.0]
Ideal: [1.0]
Actual: [0.7549937892853826]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #1)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 1.1254259630241081 || 0.7549937892853826 || 0.04532079986594939<br>|-<br>| Hidden 1 || -0.5313402159445314 || 0.3702043582229371 || -0.002408353357272021<br>|-<br>| Hidden 2 || 1.046283059531423 || 0.7400605060691366 || 0.005071619394242543<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 1.0 || 0.004975215695968892<br>|-<br>| Input 2 || 0.0 || 0.0 || 0.001802942089237876<br>|-<br>| Input Bias || N/A || 1.0 || 0.0016107887535417506<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #1)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || 0.01677795762852397<br>|-<br>| Gradient 1: H2-&gt;O1 || 0.03354013408425257<br>|-<br>| Gradient 2: B2-&gt;O1 || 0.04532079986594939<br>|-<br>| Gradient 3: I1-&gt;H1 || -0.002408353357272021<br>|-<br>| Gradient 4: I2-&gt;H1 || 0.0<br>|-<br>| Gradient 5: B1-&gt;H1 || -0.002408353357272021<br>|-<br>| Gradient 6: I1-&gt;H2 || 0.005071619394242543<br>|-<br>| Gradient 7: I2-&gt;H2 || 0.0<br>|-<br>| Gradient 8: B1-&gt;H2 || 0.005071619394242543<br>|}</p>
<p>‘’’Training Element #2’’’</p>
<pre>Input: [0.0, 0.0]
Ideal: [0.0]
Actual: [0.7303329742286414]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #2)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 0.9963125991766215 || 0.7303329742286414 || -0.14383668450008402<br>|-<br>| Hidden 1 || -0.4635107399577998 || 0.3861533099243043 || 0.007770890822508153<br>|-<br>| Hidden 2 || 0.09750161997450091 || 0.5243561128008467 || -0.020868321034070617<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 0.0 || -0.020326571124282346<br>|-<br>| Input 2 || 0.0 || 0.0 || -0.00789644741391124<br>|-<br>| Input Bias || N/A || 1.0 || -0.005636586462241866<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #2)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || -0.05554301180824532<br>|-<br>| Gradient 1: H2-&gt;O1 || -0.07542164476262586<br>|-<br>| Gradient 2: B2-&gt;O1 || -0.14383668450008402<br>|-<br>| Gradient 3: I1-&gt;H1 || 0.0<br>|-<br>| Gradient 4: I2-&gt;H1 || 0.0<br>|-<br>| Gradient 5: B1-&gt;H1 || 0.007770890822508153<br>|-<br>| Gradient 6: I1-&gt;H2 || 0.0<br>|-<br>| Gradient 7: I2-&gt;H2 || 0.0<br>|-<br>| Gradient 8: B1-&gt;H2 || -0.020868321034070617<br>|}</p>
<p>‘’’Training Element #3’’’</p>
<pre>Input: [0.0, 1.0]
Ideal: [1.0]
Actual: [0.7405954365271327]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #3)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 1.0490656424572 || 0.7405954365271327 || 0.049835205744527086<br>|-<br>| Hidden 1 || -0.24009996797891797 || 0.4402617153429706 || -0.0027990693120837377<br>|-<br>| Hidden 2 || 0.5590887364370489 || 0.6362416652701147 || 0.0067093570643624555<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 0.0 || 0.0065555728587163966<br>|-<br>| Input 2 || 0.0 || 1.0 || 0.0024716105448216665<br>|-<br>| Input Bias || N/A || 1.0 || 0.0019515718707998043<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #3)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || 0.021940533165555356<br>|-<br>| Gradient 1: H2-&gt;O1 || 0.0317072342919767<br>|-<br>| Gradient 2: B2-&gt;O1 || 0.049835205744527086<br>|-<br>| Gradient 3: I1-&gt;H1 || 0.0<br>|-<br>| Gradient 4: I2-&gt;H1 || -0.0027990693120837377<br>|-<br>| Gradient 5: B1-&gt;H1 || -0.0027990693120837377<br>|-<br>| Gradient 6: I1-&gt;H2 || 0.0<br>|-<br>| Gradient 7: I2-&gt;H2 || 0.0067093570643624555,<br>|-<br>| Gradient 8: B1-&gt;H2 || 0.0067093570643624555<br>|}</p>
<p>‘’’Training Element #4’’’</p>
<pre>Input: [1.0, 1.0]
Ideal: [0.0]
Actual: [0.7611552402539651]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #4)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 1.1590235320580868 || 0.7611552402539651 || -0.13837645506973884<br>|-<br>| Hidden 1 || -0.3079294439656496 || 0.4236202183981808 ||  0.007700680091276262<br>|-<br>| Hidden 2 || 1.507870175993971 || 0.8187453525269879 || -0.011945650103177243<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 1.0 || -0.011856144196668526<br>|-<br>| Input 2 || 0.0 || 1.0 || -0.0037935433014416875<br>|-<br>| Input Bias || N/A || 1.0 || -0.004734068163994102<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #4)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || -0.05861906411780882<br>|-<br>| Gradient 1: H2-&gt;O1 || -0.11329507948750822<br>|-<br>| Gradient 2: B2-&gt;O1 || -0.13837645506973884<br>|-<br>| Gradient 3: I1-&gt;H1 || 0.007700680091276262<br>|-<br>| Gradient 4: I2-&gt;H1 || 0.007700680091276262<br>|-<br>| Gradient 5: B1-&gt;H1 || 0.007700680091276262<br>|-<br>| Gradient 6: I1-&gt;H2 || -0.011945650103177243<br>|-<br>| Gradient 7: I2-&gt;H2 || -0.011945650103177243<br>|-<br>| Gradient 8: B1-&gt;H2 || -0.011945650103177243<br>|}</p>
<p>‘’’Batching Training Elements Together’’’</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Batch Gradients (sum of 4 training set elements)<br>|-<br>!Node || Gradient<br>|-<br>| Batch Gradient 0: H1-&gt;O1 || -0.07544358513197481<br>|-<br>| Batch Gradient 1: H2-&gt;O1 || -0.12346935587390481<br>|-<br>| Batch Gradient 2: B2-&gt;O1 || -0.18705713395934637<br>|-<br>| Batch Gradient 3: I1-&gt;H1 || 0.005292326734004241<br>|-<br>| Batch Gradient 4: I2-&gt;H1 || 0.0049016107791925246<br>|-<br>| Batch Gradient 5: B1-&gt;H1 || 0.010264148244428655<br>|-<br>| Batch Gradient 6: I1-&gt;H2 || -0.0068740307089347<br>|-<br>| Batch Gradient 7: I2-&gt;H2 || -0.005236293038814788<br>|-<br>| Batch Gradient 8: B1-&gt;H2 || -0.02103299467864286<br>|}</p>
<p>====Iteration 1:Update Weights====</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Batch Gradients (sum of 4 training set elements)<br>|-<br>!Weight || Delta t || Delta t-1 || New Weight<br>|-<br>| Batch Gradient 0: H1-&gt;O1 || -0.052810509592382364 || 0 || -0.2807299990235586<br>|-<br>| Batch Gradient 1: H2-&gt;O1 || -0.08642854911173337 || 0 || 0.49528555052962364<br>|-<br>| Batch Gradient 2: B2-&gt;O1 || -0.13093999377154245 || 0 || 0.648359126595799<br>|-<br>| Batch Gradient 3: I1-&gt;H1 || 0.0037046287138029683 || 0 || -0.06412484727292865<br>|-<br>| Batch Gradient 4: I2-&gt;H1 || 0.0034311275454347668 || 0 || 0.22684189952431658<br>|-<br>| Batch Gradient 5: B1-&gt;H1 || 0.007184903771100058 || 0 || -0.45632583618669975<br>|-<br>| Batch Gradient 6: I1-&gt;H2 || -0.004811821496254289 || 0 || 0.9439696180606678<br>|-<br>| Batch Gradient 7: I2-&gt;H2 || -0.003665405127170351 || 0 || 0.4579217113353777<br>|-<br>| Batch Gradient 8: B1-&gt;H2 || -0.014723096275050002 || 0 || 0.0827785236994509<br>|}</p>
<p>===Iteration 2===</p>
<p>====Iteration 2:Calculate Gradients====<br>‘’’Training Element #1’’’</p>
<pre>Input: [1.0, 0.0]
Ideal: [1.0]
Actual: [0.7126704320302026]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #1)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 0.90838920899152 || 0.7126704320302026 || 0.05883684552404642<br>|-<br>| Hidden 1 || -0.5204506834596284 || 0.3727468548609481 || -0.003861846787694659<br>|-<br>| Hidden 2 || 1.0267481417601187 || 0.7362849697652507 || 0.005658298521549234<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 1.0 || 0.005588902229712442<br>|-<br>| Input 2 || 0.0 || 0.0 || 0.001715029081241726<br>|-<br>| Input Bias || N/A || 1.0 || 0.002230646062884317<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #1)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || 0.021931249119027756<br>|-<br>| Gradient 1: H2-&gt;O1 || 0.04332068502775525<br>|-<br>| Gradient 2: B2-&gt;O1 || 0.05883684552404642<br>|-<br>| Gradient 3: I1-&gt;H1 || -0.003861846787694659<br>|-<br>| Gradient 4: I2-&gt;H1 || -0.0<br>|-<br>| Gradient 5: B1-&gt;H1 || -0.003861846787694659<br>|-<br>| Gradient 6: I1-&gt;H2 || 0.005658298521549234<br>|-<br>| Gradient 7: I2-&gt;H2 || 0.0<br>|-<br>| Gradient 8: B1-&gt;H2 || 0.005658298521549234<br>|}</p>
<p>‘’’Training Element #2’’’</p>
<pre>Input: [0.0, 0.0]
Ideal: [0.0]
Actual: [0.6894100086355014]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #2)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 0.7973624852278878 || 0.6894100086355014 || -0.14761912433217836<br>|-<br>| Hidden 1 || -0.45632583618669975 || 0.3878577987356633 || 0.009839120925080193<br>|-<br>| Hidden 2 || 0.0827785236994509 || 0.5206828218926742 || -0.018247128336076817<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 0.0 || -0.01785566689273107<br>|-<br>| Input 2 || 0.0 || 0.0 || -0.006123831354317917<br>|-<br>| Input Bias || N/A || 1.0 || -0.006000315428894131<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #2)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || -0.05725522861476489<br>|-<br>| Gradient 1: H2-&gt;O1 || -0.07686274222260416<br>|-<br>| Gradient 2: B2-&gt;O1 || -0.14761912433217836<br>|-<br>| Gradient 3: I1-&gt;H1 || 0.0<br>|-<br>| Gradient 4: I2-&gt;H1 || 0.0<br>|-<br>| Gradient 5: B1-&gt;H1 || 0.009839120925080193<br>|-<br>| Gradient 6: I1-&gt;H2 || 0.0<br>|-<br>| Gradient 7: I2-&gt;H2 || 0.0<br>|-<br>| Gradient 8: B1-&gt;H2 || -0.018247128336076817<br>|}</p>
<p>‘’’Training Element #3’’’</p>
<pre>Input: [0.0, 1.0]
Ideal: [1.0]
Actual: [0.6978409768403965]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #3)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 0.837037804388766 || 0.6978409768403965 || 0.06371293371673999<br>|-<br>| Hidden 1 || -0.22948393666238318 || 0.44287947369167335 || -0.0044131748974781376<br>|-<br>| Hidden 2 || 0.5407002350348286 || 0.6319752952115714 || 0.007339396247406085<br>|-<br>| Hidden Bias || N/A || 1.0 || *0.0<br>|-<br>| Input 1 || 0.0 || 0.0 || 0.007211161238749329<br>|-<br>| Input 2 || 0.0 || 1.0 || 0.002359775913103672<br>|-<br>| Input Bias || N/A || 1.0 || 0.0026213901115354297<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #3)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || 0.028217150551822275<br>|-<br>| Gradient 1: H2-&gt;O1 || 0.040265000094432034<br>|-<br>| Gradient 2: B2-&gt;O1 || 0.06371293371673999<br>|-<br>| Gradient 3: I1-&gt;H1 || 0.0<br>|-<br>| Gradient 4: I2-&gt;H1 || -0.0044131748974781376<br>|<br>| Gradient 5: B1-&gt;H1 || -0.0044131748974781376<br>|-<br>| Gradient 6: I1-&gt;H2 || 0.0<br>|-<br>| Gradient 7: I2-&gt;H2 || 0.007339396247406085<br>|-<br>| Gradient 8: B1-&gt;H2 || 0.007339396247406085<br>|}</p>
<p>‘’’Training Element #4’’’</p>
<pre>Input: [1.0, 1.0]
Ideal: [0.0]
Actual: [0.7175312197386368]</pre>

<p>{| class=”wikitable” style=”text-align:left”<br>|+ Network Calculation (Training Element #4)<br>|-<br>!Node || Sum || Output || Node Delta<br>|-<br>| Output 1 || 0.9322484263876174 || 0.7175312197386368 || -0.14542934847688305<br>|-<br>| Hidden 1 ||-0.2936087839353118 || 0.4271206074568061 ||  0.009989749735560847<br>|-<br>| Hidden 2 || 1.4846698530954965 || 0.8152768984968762 || -0.010847610049093622<br>|-<br>| Hidden Bias || N/A || 1.0 || 0.0<br>|-<br>| Input 1 || 0.0 || 1.0 || -0.010880405491001586<br>|-<br>| Input 2 || 0.0 || 1.0 || -0.0027012623517926305<br>|-<br>| Input Bias || N/A || 1.0 || -0.005456530046906965<br>|}</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Gradient Calculation (Training Element #4)<br>|-<br>!Node || Gradient<br>|-<br>| Gradient 0: H1-&gt;O1 || -0.062115871663493825<br>|-<br>| Gradient 1: H2-&gt;O1 || -0.11856518817665462<br>|-<br>| Gradient 2: B2-&gt;O1 || -0.14542934847688305<br>|-<br>| Gradient 3: I1-&gt;H1 || 0.009989749735560847<br>|-<br>| Gradient 4: I2-&gt;H1 || 0.009989749735560847<br>|-<br>| Gradient 5: B1-&gt;H1 || 0.009989749735560847<br>|-<br>| Gradient 6: I1-&gt;H2 || -0.010847610049093622<br>|-<br>| Gradient 7: I2-&gt;H2 || -0.010847610049093622<br>|-<br>| Gradient 8: B1-&gt;H2 || -0.010847610049093622<br>|}</p>
<p>‘’’Batching Training Elements Together’’’</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Batch Gradients (sum of 4 training set elements)<br>|-<br>!Node || Gradient<br>|-<br>| Batch Gradient 0: H1-&gt;O1 || -0.06922270060740869<br>|-<br>| Batch Gradient 1: H2-&gt;O1 || -0.1118422452770715<br>|-<br>| Batch Gradient 2: B2-&gt;O1 || -0.17049869356827502<br>|-<br>| Batch Gradient 3: I1-&gt;H1 || 0.0061279029478661885<br>|-<br>| Batch Gradient 4: I2-&gt;H1 || 0.00557657483808271<br>|-<br>| Batch Gradient 5: B1-&gt;H1 || 0.011553848975468243<br>|-<br>| Batch Gradient 6: I1-&gt;H2 || -0.005189311527544388<br>|-<br>| Batch Gradient 7: I2-&gt;H2 || -0.003508213801687537<br>|-<br>| Batch Gradient 8: B1-&gt;H2 || -0.01609704361621512<br>|}</p>
<p>====Iteration 2:Update Weights====</p>
<p>{| class=”wikitable” style=”text-align:left”<br>|+ Batch Gradients (sum of 4 training set elements)<br>|-<br>!Weight || Delta t || Delta t-1 || New Weight<br>|-<br>| Batch Gradient 0: H1-&gt;O1 || -0.06429904330290079 || -0.052810509592382364 || -0.34502904232645937<br>|-<br>| Batch Gradient 1: H2-&gt;O1 || -0.10421813642747005 || -0.08642854911173337 || 0.3910674141021536<br>|-<br>| Batch Gradient 2: B2-&gt;O1 || -0.15863108362925524 || -0.13093999377154245 || 0.4897280429665437<br>|-<br>| Batch Gradient 3: I1-&gt;H1 || 0.005400920677647222 || 0.0037046287138029683 || -0.05872392659528143<br>|-<br>| Batch Gradient 4: I2-&gt;H1 || 0.004932940650288327 || 0.0034311275454347668 || 0.2317748401746049<br>|-<br>| Batch Gradient 5: B1-&gt;H1 || 0.010243165414157786 || 0.007184903771100058 || -0.446082670772542<br>|-<br>| Batch Gradient 6: I1-&gt;H2 || -0.005076064518157358 || -0.004811821496254289 || 0.9388935535425105<br>|-<br>| Batch Gradient 7: I2-&gt;H2 || -0.003555371199332381 || -0.003665405127170351 || 0.4543663401360453<br>|-<br>| Batch Gradient 8: B1-&gt;H2 || -0.015684859413865583 || -0.014723096275050002 || 0.06709366428558533<br>|}</p>
<p>===Other Iterations===<br>We have seen the first two iterations.  It takes additional iterations before the error level of the neural network drops to below 1%.  These iterations are shown here.</p>
<p><pre>Epoch #1 Error:0.3100155809627523<br>Epoch #2 Error:0.2909988918032235<br>Epoch #3 Error:0.2712902750837602<br>Epoch #4 Error:0.2583119003843881<br>Epoch #5 Error:0.2523050561276289<br>Epoch #6 Error:0.2502986971902545<br>Epoch #7 Error:0.2498182295192154<br>Epoch #8 Error:0.24974245650541688<br>Epoch #9 Error:0.24973458893806627<br>Epoch #10 Error:0.24972923906975902<br>Epoch #11 Error:0.24972189153837634<br>Epoch #12 Error:0.24971434375093543<br>Epoch #13 Error:0.24970710134743646<br>Epoch #14 Error:0.24970001601001643<br>Epoch #15 Error:0.24969291819530431<br>Epoch #16 Error:0.2496857298315751<br>Epoch #17 Error:0.24967842574830518<br>Epoch #18 Error:0.2496709985253335<br>Epoch #19 Error:0.2496634445301509<br>Epoch #20 Error:0.24965576041167767<br>Epoch #21 Error:0.24964794264095624<br>Epoch #22 Error:0.24963998755908973<br>Epoch #23 Error:0.2496318914148358<br>Epoch #24 Error:0.24962365036767536<br>Epoch #25 Error:0.24961526048198857<br>Epoch #26 Error:0.24960671772194287<br>Epoch #27 Error:0.2495980179477877<br>Epoch #28 Error:0.24958915691268324<br>Epoch #29 Error:0.24958013025956088<br>Epoch #30 Error:0.24957093351787052<br>Epoch #31 Error:0.24956156210020647<br>Epoch #32 Error:0.24955201129882154<br>Epoch #33 Error:0.24954227628204287<br>Epoch #34 Error:0.2495323520905876<br>Epoch #35 Error:0.24952223363378223<br>Epoch #36 Error:0.2495119156856801<br>Epoch #37 Error:0.24950139288107853<br>Epoch #38 Error:0.2494906597114292<br>Epoch #39 Error:0.24947971052064272<br>Epoch #40 Error:0.24946853950078324<br>Epoch #41 Error:0.24945714068765068<br>Epoch #42 Error:0.2494455079562504<br>Epoch #43 Error:0.24943363501614585<br>Epoch #44 Error:0.24942151540669405<br>Epoch #45 Error:0.24940914249216084<br>Epoch #46 Error:0.2493965094567142<br>Epoch #47 Error:0.24938360929929476<br>Epoch #48 Error:0.24937043482836047<br>Epoch #49 Error:0.24935697865650427<br>Epoch #50 Error:0.24934323319494384<br>Epoch #51 Error:0.2493291906478809<br>Epoch #52 Error:0.24931484300672974<br>Epoch #53 Error:0.24930018204421345<br>Epoch #54 Error:0.24928519930832602<br>Epoch #55 Error:0.24926988611616174<br>Epoch #56 Error:0.24925423354760812<br>Epoch #57 Error:0.2492382324389047<br>Epoch #58 Error:0.24922187337606533<br>Epoch #59 Error:0.24920514668816585<br>Epoch #60 Error:0.24918804244049497<br>Epoch #61 Error:0.24917055042757214<br>Epoch #62 Error:0.2491526601660287<br>Epoch #63 Error:0.24913436088735835<br>Epoch #64 Error:0.2491156415305345<br>Epoch #65 Error:0.2490964907344973<br>Epoch #66 Error:0.2490768968305138<br>Epoch #67 Error:0.2490568478344118<br>Epoch #68 Error:0.24903633143869064<br>Epoch #69 Error:0.24901533500451387<br>Epoch #70 Error:0.24899384555358464<br>Epoch #71 Error:0.24897184975991077<br>Epoch #72 Error:0.24894933394146201<br>Epoch #73 Error:0.2489262840517267<br>Epoch #74 Error:0.24890268567117108<br>Epoch #75 Error:0.24887852399860996<br>Epoch #76 Error:0.2488537838424939<br>Epoch #77 Error:0.24882844961212114<br>Epoch #78 Error:0.24880250530878126<br>Epoch #79 Error:0.2487759345168411<br>Epoch #80 Error:0.24874872039477997<br>Epoch #81 Error:0.24872084566618513<br>Epoch #82 Error:0.24869229261071904<br>Epoch #83 Error:0.24866304305506665<br>Epoch #84 Error:0.24863307836387832<br>Epoch #85 Error:0.24860237943071778<br>Epoch #86 Error:0.24857092666903038<br>Epoch #87 Error:0.24853870000314368<br>Epoch #88 Error:0.24850567885931676<br>Epoch #89 Error:0.2484718421568517<br>Epoch #90 Error:0.2484371682992842<br>Epoch #91 Error:0.2484016351656681<br>Epoch #92 Error:0.2483652201019731<br>Epoch #93 Error:0.24832789991260973<br>Epoch #94 Error:0.24828965085210292<br>Epoch #95 Error:0.24825044861692888<br>Epoch #96 Error:0.24821026833753734<br>Epoch #97 Error:0.24816908457057552<br>Epoch #98 Error:0.24812687129133337<br>Epoch #99 Error:0.2480836018864306<br>Epoch #100 Error:0.24803924914676378<br>Epoch #101 Error:0.24799378526073232<br>Epoch #102 Error:0.24794718180776376<br>Epoch #103 Error:0.24789940975215627<br>Epoch #104 Error:0.247850439437257<br>Epoch #105 Error:0.2478002405799949<br>Epoch #106 Error:0.24774878226578395<br>Epoch #107 Error:0.24769603294381562<br>Epoch #108 Error:0.247641960422753<br>Epoch #109 Error:0.2475865318668452<br>Epoch #110 Error:0.24752971379247174<br>Epoch #111 Error:0.24747147206513093<br>Epoch #112 Error:0.24741177189688257<br>Epoch #113 Error:0.24735057784425257<br>Epoch #114 Error:0.24728785380660845<br>Epoch #115 Error:0.24722356302500836<br>Epoch #116 Error:0.2471576680815282<br>Epoch #117 Error:0.2470901308990665<br>Epoch #118 Error:0.24702091274162513<br>Epoch #119 Error:0.24694997421506196<br>Epoch #120 Error:0.24687727526830808<br>Epoch #121 Error:0.2468027751950393<br>Epoch #122 Error:0.24672643263579042<br>Epoch #123 Error:0.2466482055804965<br>Epoch #124 Error:0.246568051371442<br>Epoch #125 Error:0.24648592670659816<br>Epoch #126 Error:0.24640178764332415<br>Epoch #127 Error:0.2463155896024049<br>Epoch #128 Error:0.24622728737239746<br>Epoch #129 Error:0.24613683511425338<br>Epoch #130 Error:0.24604418636618436<br>Epoch #131 Error:0.24594929404873317<br>Epoch #132 Error:0.245852110470015<br>Epoch #133 Error:0.2457525873310876<br>Epoch #134 Error:0.24565067573141125<br>Epoch #135 Error:0.24554632617435784<br>Epoch #136 Error:0.2454394885727269<br>Epoch #137 Error:0.24533011225422965<br>Epoch #138 Error:0.24521814596689887<br>Epoch #139 Error:0.2451035378843885<br>Epoch #140 Error:0.24498623561112431<br>Epoch #141 Error:0.24486618618727468<br>Epoch #142 Error:0.24474333609350843<br>Epoch #143 Error:0.24461763125551564<br>Epoch #144 Error:0.24448901704827075<br>Epoch #145 Error:0.2443574383000207<br>Epoch #146 Error:0.24422283929599173<br>Epoch #147 Error:0.24408516378181133<br>Epoch #148 Error:0.24394435496665412<br>Epoch #149 Error:0.24380035552612614<br>Epoch #150 Error:0.2436531076049127<br>Epoch #151 Error:0.24350255281922725<br>Epoch #152 Error:0.2433486322591068<br>Epoch #153 Error:0.2431912864906141<br>Epoch #154 Error:0.24303045555801814<br>Epoch #155 Error:0.2428660789860364<br>Epoch #156 Error:0.24269809578224028<br>Epoch #157 Error:0.24252644443973276<br>Epoch #158 Error:0.2423510629402286<br>Epoch #159 Error:0.2421718887576766<br>Epoch #160 Error:0.24198885886258256<br>Epoch #161 Error:0.24180190972720367<br>Epoch #162 Error:0.24161097733180137<br>Epoch #163 Error:0.24141599717215478<br>Epoch #164 Error:0.2412169042685502<br>Epoch #165 Error:0.24101363317647734<br>Epoch #166 Error:0.24080611799927562<br>Epoch #167 Error:0.24059429240298655<br>Epoch #168 Error:0.24037808963368124<br>Epoch #169 Error:0.24015744253754098<br>Epoch #170 Error:0.23993228358398<br>Epoch #171 Error:0.2397025448921065<br>Epoch #172 Error:0.23946815826082551<br>Epoch #173 Error:0.23922905520289078<br>Epoch #174 Error:0.23898516698321748<br>Epoch #175 Error:0.23873642466176542<br>Epoch #176 Error:0.23848275914130418<br>Epoch #177 Error:0.23822410122036458<br>Epoch #178 Error:0.2379603816516747<br>Epoch #179 Error:0.23769153120637032<br>Epoch #180 Error:0.2374174807442544<br>Epoch #181 Error:0.23713816129036458<br>Epoch #182 Error:0.23685350411808948<br>Epoch #183 Error:0.23656344083904915<br>Epoch #184 Error:0.23626790349992755<br>Epoch #185 Error:0.23596682468641644<br>Epoch #186 Error:0.23566013763439095<br>Epoch #187 Error:0.23534777634840032<br>Epoch #188 Error:0.2350296757275127<br>Epoch #189 Error:0.23470577169850368<br>Epoch #190 Error:0.23437600135632652<br>Epoch #191 Error:0.23404030311174687<br>Epoch #192 Error:0.2336986168459597<br>Epoch #193 Error:0.23335088407194515<br>Epoch #194 Error:0.23299704810224886<br>Epoch #195 Error:0.23263705422280045<br>Epoch #196 Error:0.23227084987230728<br>Epoch #197 Error:0.23189838482668362<br>Epoch #198 Error:0.23151961138789198<br>Epoch #199 Error:0.231134484576493<br>Epoch #200 Error:0.23074296232711494<br>Epoch #201 Error:0.2303450056859696<br>Epoch #202 Error:0.2299405790094592<br>Epoch #203 Error:0.2295296501628348<br>Epoch #204 Error:0.2291121907177881<br>Epoch #205 Error:0.22868817614778236<br>Epoch #206 Error:0.22825758601985507<br>Epoch #207 Error:0.22782040418156102<br>Epoch #208 Error:0.22737661894166425<br>Epoch #209 Error:0.2269262232431381<br>Epoch #210 Error:0.22646921482698917<br>Epoch #211 Error:0.22600559638539408<br>Epoch #212 Error:0.22553537570261567<br>Epoch #213 Error:0.2250585657821631<br>Epoch #214 Error:0.22457518495866538<br>Epoch #215 Error:0.22408525699295367<br>Epoch #216 Error:0.22358881114888354<br>Epoch #217 Error:0.2230858822504846<br>Epoch #218 Error:0.2225765107180943<br>Epoch #219 Error:0.22206074258221997<br>Epoch #220 Error:0.22153862947397523<br>Epoch #221 Error:0.221010228591058<br>Epoch #222 Error:0.22047560263836652<br>Epoch #223 Error:0.2199348197425032<br>Epoch #224 Error:0.21938795333957137<br>Epoch #225 Error:0.21883508203584606<br>Epoch #226 Error:0.21827628944107985<br>Epoch #227 Error:0.21771166397439418<br>Epoch #228 Error:0.2171412986429027<br>Epoch #229 Error:0.21656529079341202<br>Epoch #230 Error:0.21598374183774435<br>Epoch #231 Error:0.21539675695242494<br>Epoch #232 Error:0.21480444475367338<br>Epoch #233 Error:0.21420691694882488<br>Epoch #234 Error:0.2136042879654871<br>Epoch #235 Error:0.21299667455991023<br>Epoch #236 Error:0.21238419540619963<br>Epoch #237 Error:0.21176697066814593<br>Epoch #238 Error:0.21114512155556997<br>Epoch #239 Error:0.2105187698671867<br>Epoch #240 Error:0.20988803752208296<br>Epoch #241 Error:0.2092530460819696<br>Epoch #242 Error:0.2086139162664173<br>Epoch #243 Error:0.2079707674633161<br>Epoch #244 Error:0.20732371723680276<br>Epoch #245 Error:0.20667288083489005<br>Epoch #246 Error:0.2060183706990029<br>Epoch #247 Error:0.2053602959775751<br>Epoch #248 Error:0.2046987620457984<br>Epoch #249 Error:0.20403387003353274<br>Epoch #250 Error:0.20336571636329595<br>Epoch #251 Error:0.2026943923001445<br>Epoch #252 Error:0.2020199835151432<br>Epoch #253 Error:0.2013425696639975<br>Epoch #254 Error:0.20066222398229702<br>Epoch #255 Error:0.19997901289868375<br>Epoch #256 Error:0.1992929956671255<br>Epoch #257 Error:0.1986042240193414<br>Epoch #258 Error:0.19791274183829527<br>Epoch #259 Error:0.19721858485353888<br>Epoch #260 Error:0.19652178035906992<br>Epoch #261 Error:0.195822346954247<br>Epoch #262 Error:0.19512029430819378<br>Epoch #263 Error:0.19441562294802792<br>Epoch #264 Error:0.1937083240711511<br>Epoch #265 Error:0.1929983793817608<br>Epoch #266 Error:0.19228576095167016<br>Epoch #267 Error:0.19157043110546443<br>Epoch #268 Error:0.19085234232997111<br>Epoch #269 Error:0.19013143720799042<br>Epoch #270 Error:0.18940764837619845<br>Epoch #271 Error:0.18868089850713027<br>Epoch #272 Error:0.1879511003151419<br>Epoch #273 Error:0.187218156586258<br>Epoch #274 Error:0.18648196023183178<br>Epoch #275 Error:0.1857423943659678<br>Epoch #276 Error:0.18499933240669536<br>Epoch #277 Error:0.1842526382009208<br>Epoch #278 Error:0.18350216617323836<br>Epoch #279 Error:0.18274776149873317<br>Epoch #280 Error:0.18198926029996984<br>Epoch #281 Error:0.18122648986842338<br>Epoch #282 Error:0.1804592689106738<br>Epoch #283 Error:0.1796874078197531<br>Epoch #284 Error:0.1789107089720991<br>Epoch #285 Error:0.17812896705063383<br>Epoch #286 Error:0.17734196939454772<br>Epoch #287 Error:0.17654949637642608<br>Epoch #288 Error:0.17575132180740657<br>Epoch #289 Error:0.1749472133711008<br>Epoch #290 Error:0.17413693308704556<br>Epoch #291 Error:0.1733202378044792<br>Epoch #292 Error:0.1724968797272453<br>Epoch #293 Error:0.17166660697063274<br>Epoch #294 Error:0.1708291641509393<br>Epoch #295 Error:0.16998429300851856<br>Epoch #296 Error:0.16913173306502072<br>Epoch #297 Error:0.1682712223154657<br>Epoch #298 Error:0.16740249795570145<br>Epoch #299 Error:0.16652529714568398<br>Epoch #300 Error:0.16563935780888592<br>Epoch #301 Error:0.1647444194679728<br>Epoch #302 Error:0.1638402241167062<br>Epoch #303 Error:0.162926517127816<br>Epoch #304 Error:0.16200304819634356<br>Epoch #305 Error:0.16106957231768845<br>Epoch #306 Error:0.160125850799293<br>Epoch #307 Error:0.1591716523045697<br>Epoch #308 Error:0.1582067539273233<br>Epoch #309 Error:0.15723094229453444<br>Epoch #310 Error:0.1562440146949579<br>Epoch #311 Error:0.1552457802305584<br>Epoch #312 Error:0.1542360609873409<br>Epoch #313 Error:0.15321469322165795<br>Epoch #314 Error:0.152181528557576<br>Epoch #315 Error:0.15113643519037823<br>Epoch #316 Error:0.15007929909075807<br>Epoch #317 Error:0.1490100252037463<br>Epoch #318 Error:0.1479285386358941<br>Epoch #319 Error:0.14683478582373807<br>Epoch #320 Error:0.14572873567608863<br>Epoch #321 Error:0.1446103806822327<br>Epoch #322 Error:0.14347973797773195<br>Epoch #323 Error:0.14233685035913068<br>Epoch #324 Error:0.14118178723859034<br>Epoch #325 Error:0.14001464552923407<br>Epoch #326 Error:0.1388355504518368<br>Epoch #327 Error:0.13764465625344605<br>Epoch #328 Error:0.13644214682855943<br>Epoch #329 Error:0.1352282362336583<br>Epoch #330 Error:0.1340031690861708<br>Epoch #331 Error:0.13276722083935977<br>Epoch #332 Error:0.13152069792517324<br>Epoch #333 Error:0.13026393775778092<br>Epoch #334 Error:0.12899730859134637<br>Epoch #335 Error:0.12772120922653737<br>Epoch #336 Error:0.12643606856137407<br>Epoch #337 Error:0.12514234498322174<br>Epoch #338 Error:0.12384052560006499<br>Epoch #339 Error:0.12253112531062212<br>Epoch #340 Error:0.12121468571436363<br>Epoch #341 Error:0.11989177386406517<br>Epoch #342 Error:0.1185629808651315<br>Epoch #343 Error:0.11722892032754417<br>Epoch #344 Error:0.115890226677898<br>Epoch #345 Error:0.11454755334055572<br>Epoch #346 Error:0.11320157079845716<br>Epoch #347 Error:0.11185296454552558<br>Epoch #348 Error:0.11050243294390527<br>Epoch #349 Error:0.10915068500040971<br>Epoch #350 Error:0.10779843807753928<br>Epoch #351 Error:0.10644641555521947<br>Epoch #352 Error:0.1050953444600049<br>Epoch #353 Error:0.10374595307887113<br>Epoch #354 Error:0.10239896857487604<br>Epoch #355 Error:0.10105511462190382<br>Epoch #356 Error:0.09971510907541956<br>Epoch #357 Error:0.09837966169565446<br>Epoch #358 Error:0.09704947193893299<br>Epoch #359 Error:0.0957252268319504<br>Epoch #360 Error:0.09440759894273237<br>Epoch #361 Error:0.0930972444607828<br>Epoch #362 Error:0.0917948013975674<br>Epoch #363 Error:0.09050088791702199<br>Epoch #364 Error:0.08921610080423807<br>Epoch #365 Error:0.08794101407889429<br>Epoch #366 Error:0.0866761777583944<br>Epoch #367 Error:0.08542211677407002<br>Epoch #368 Error:0.08417933004223217<br>Epoch #369 Error:0.08294828969033477<br>Epoch #370 Error:0.08172944043706307<br>Epoch #371 Error:0.08052319912380262<br>Epoch #372 Error:0.07932995439369152<br>Epoch #373 Error:0.07815006651332447<br>Epoch #374 Error:0.07698386733117121<br>Epoch #375 Error:0.07583166036589747<br>Epoch #376 Error:0.07469372101703958<br>Epoch #377 Error:0.07357029688988702<br>Epoch #378 Error:0.07246160822595926<br>Epoch #379 Error:0.07136784843013287<br>Epoch #380 Error:0.07028918468526468<br>Epoch #381 Error:0.06922575864506868<br>Epoch #382 Error:0.06817768719601615<br>Epoch #383 Error:0.06714506327915051<br>Epoch #384 Error:0.06612795676290573<br>Epoch #385 Error:0.06512641535829994<br>Epoch #386 Error:0.06414046556822048<br>Epoch #387 Error:0.06317011366291518<br>Epoch #388 Error:0.062215346674250493<br>Epoch #389 Error:0.06127613340177227<br>Epoch #390 Error:0.06035242542410814<br>Epoch #391 Error:0.05944415810976657<br>Epoch #392 Error:0.05855125162191316<br>Epoch #393 Error:0.05767361191222686<br>Epoch #394 Error:0.056811131699460296<br>Epoch #395 Error:0.05596369142883201<br>Epoch #396 Error:0.055131160208870746<br>Epoch #397 Error:0.054313396722801624<br>Epoch #398 Error:0.053510250112010306<br>Epoch #399 Error:0.05272156082954427<br>Epoch #400 Error:0.05194716146200465<br>Epoch #401 Error:0.0511868775185465<br>Epoch #402 Error:0.05044052818604555<br>Epoch #403 Error:0.04970792704979528<br>Epoch #404 Error:0.04898888277938031<br>Epoch #405 Error:0.04828319977962167<br>Epoch #406 Error:0.04759067880671464<br>Epoch #407 Error:0.046911117549880005<br>Epoch #408 Error:0.046244311179019684<br>Epoch #409 Error:0.04559005285902118<br>Epoch #410 Error:0.044948134231481224<br>Epoch #411 Error:0.044318345864728<br>Epoch #412 Error:0.043700477673110125<br>Epoch #413 Error:0.04309431930659188<br>Epoch #414 Error:0.042499660511750297<br>Epoch #415 Error:0.04191629146531273<br>Epoch #416 Error:0.04134400308139992<br>Epoch #417 Error:0.04078258729366059<br>Epoch #418 Error:0.0402318373134883<br>Epoch #419 Error:0.03969154786551182<br>Epoch #420 Error:0.0391615154015402<br>Epoch #421 Error:0.03864153829412952<br>Epoch #422 Error:0.0381314170109153<br>Epoch #423 Error:0.0376309542708296<br>Epoch #424 Error:0.03713995518329316<br>Epoch #425 Error:0.03665822737143622<br>Epoch #426 Error:0.036185581080370455<br>Epoch #427 Error:0.035721829271493435<br>Epoch #428 Error:0.03526678770376991<br>Epoch #429 Error:0.03482027500289418<br>Epoch #430 Error:0.03438211271919581<br>Epoch #431 Error:0.03395212537511341<br>Epoch #432 Error:0.033530140503017274<br>Epoch #433 Error:0.03311598867412426<br>Epoch #434 Error:0.03270950351920786<br>Epoch #435 Error:0.03231052174176729<br>Epoch #436 Error:0.031918883124283204<br>Epoch #437 Error:0.0315344305281495<br>Epoch #438 Error:0.031157009887836276<br>Epoch #439 Error:0.03078647019980489<br>Epoch #440 Error:0.03042266350666263<br>Epoch #441 Error:0.030065444877013826<br>Epoch #442 Error:0.02971467238143323<br>Epoch #443 Error:0.029370207064960158<br>Epoch #444 Error:0.02903191291648297<br>Epoch #445 Error:0.028699656835358668<br>Epoch #446 Error:0.028373308595587392<br>Epoch #447 Error:0.028052740807838438<br>Epoch #448 Error:0.027737828879601866<br>Epoch #449 Error:0.027428450973720025<br>Epoch #450 Error:0.027124487965532865<br>Epoch #451 Error:0.026825823398852998<br>Epoch #452 Error:0.02653234344096876<br>Epoch #453 Error:0.026243936836858425<br>Epoch #454 Error:0.025960494862781784<br>Epoch #455 Error:0.025681911279403148<br>Epoch #456 Error:0.02540808228458449<br>Epoch #457 Error:0.025138906465977266<br>Epoch #458 Error:0.02487428475352811<br>Epoch #459 Error:0.02461412037200357<br>Epoch #460 Error:0.0243583187936298<br>Epoch #461 Error:0.024106787690932052<br>Epoch #462 Error:0.02385943688985246<br>Epoch #463 Error:0.023616178323214762<br>Epoch #464 Error:0.023376925984598087<br>Epoch #465 Error:0.023141595882675434<br>Epoch #466 Error:0.0229101059960651<br>Epoch #467 Error:0.02268237622873859<br>Epoch #468 Error:0.02245832836602291<br>Epoch #469 Error:0.02223788603122914<br>Epoch #470 Error:0.022020974642936918<br>Epoch #471 Error:0.02180752137295783<br>Epoch #472 Error:0.02159745510499847<br>Epoch #473 Error:0.021390706394040254<br>Epoch #474 Error:0.02118720742644902<br>Epoch #475 Error:0.020986891980825782<br>Epoch #476 Error:0.020789695389606188<br>Epoch #477 Error:0.020595554501414766<br>Epoch #478 Error:0.02040440764417725<br>Epoch #479 Error:0.020216194588992476<br>Epoch #480 Error:0.02003085651476362<br>Epoch #481 Error:0.019848335973586928<br>Epoch #482 Error:0.01966857685689445<br>Epoch #483 Error:0.019491524362346423<br>Epoch #484 Error:0.019317124961466908<br>Epoch #485 Error:0.0191453263680166<br>Epoch #486 Error:0.018976077507094596<br>Epoch #487 Error:0.01880932848496037<br>Epoch #488 Error:0.018645030559567337<br>Epoch #489 Error:0.01848313611179742<br>Epoch #490 Error:0.01832359861738679<br>Epoch #491 Error:0.018166372619531176<br>Epoch #492 Error:0.01801141370216045<br>Epoch #493 Error:0.01785867846387005<br>Epoch #494 Error:0.017708124492497587<br>Epoch #495 Error:0.017559710340333143<br>Epoch #496 Error:0.017413395499949955<br>Epoch #497 Error:0.017269140380644366<br>Epoch #498 Error:0.017126906285471505<br>Epoch #499 Error:0.016986655388865138<br>Epoch #500 Error:0.016848350714828417<br>Epoch #501 Error:0.016711956115683664<br>Epoch #502 Error:0.016577436251368635<br>Epoch #503 Error:0.016444756569266403<br>Epoch #504 Error:0.016313883284557584<br>Epoch #505 Error:0.01618478336108175<br>Epoch #506 Error:0.01605742449269653<br>Epoch #507 Error:0.01593177508512255<br>Epoch #508 Error:0.01580780423826206<br>Epoch #509 Error:0.01568548172898021<br>Epoch #510 Error:0.015564777994337024<br>Epoch #511 Error:0.015445664115259291<br>Epoch #512 Error:0.015328111800640892<br>Epoch #513 Error:0.015212093371861331<br>Epoch #514 Error:0.015097581747711072<br>Epoch #515 Error:0.014984550429713736<br>Epoch #516 Error:0.014872973487834885<br>Epoch #517 Error:0.01476282554656709<br>Epoch #518 Error:0.014654081771381621<br>Epoch #519 Error:0.014546717855537263<br>Epoch #520 Error:0.014440710007236528<br>Epoch #521 Error:0.014336034937120193<br>Epoch #522 Error:0.014232669846091502<br>Epoch #523 Error:0.014130592413460671<br>Epoch #524 Error:0.014029780785401397<br>Epoch #525 Error:0.013930213563711213<br>Epoch #526 Error:0.013831869794866999<br>Epoch #527 Error:0.013734728959368121<br>Epoch #528 Error:0.013638770961359351<br>Epoch #529 Error:0.0135439761185256<br>Epoch #530 Error:0.013450325152251644<br>Epoch #531 Error:0.013357799178039262<br>Epoch #532 Error:0.013266379696174726<br>Epoch #533 Error:0.013176048582640185<br>Epoch #534 Error:0.013086788080261855<br>Epoch #535 Error:0.01299858079008884<br>Epoch #536 Error:0.01291140966299595<br>Epoch #537 Error:0.012825257991504985<br>Epoch #538 Error:0.012740109401817504<br>Epoch #539 Error:0.012655947846054517<br>Epoch #540 Error:0.012572757594696116<br>Epoch #541 Error:0.012490523229216854<br>Epoch #542 Error:0.012409229634910354<br>Epoch #543 Error:0.012328861993898646<br>Epoch #544 Error:0.012249405778321096<br>Epoch #545 Error:0.01217084674369763<br>Epoch #546 Error:0.012093170922461986<br>Epoch #547 Error:0.012016364617659817<br>Epoch #548 Error:0.011940414396807536<br>Epoch #549 Error:0.01186530708590733<br>Epoch #550 Error:0.011791029763613976<br>Epoch #551 Error:0.011717569755549568<br>Epoch #552 Error:0.011644914628761726<br>Epoch #553 Error:0.011573052186321896<br>Epoch #554 Error:0.011501970462059325<br>Epoch #555 Error:0.011431657715427491<br>Epoch #556 Error:0.011362102426499139<br>Epoch #557 Error:0.011293293291086408<br>Epoch #558 Error:0.01122521921598292<br>Epoch #559 Error:0.011157869314324005<br>Epoch #560 Error:0.01109123290106263<br>Epoch #561 Error:0.01102529948855689<br>Epoch #562 Error:0.01096005878226718<br>Epoch #563 Error:0.010895500676558947<br>Epoch #564 Error:0.010831615250609324<br>Epoch #565 Error:0.010768392764413662<br>Epoch #566 Error:0.010705823654890458<br>Epoch #567 Error:0.010643898532081014<br>Epoch #568 Error:0.010582608175441923<br>Epoch #569 Error:0.010521943530227711<br>Epoch #570 Error:0.010461895703961211<br>Epoch #571 Error:0.010402455962989346<br>Epoch #572 Error:0.010343615729122041<br>Epoch #573 Error:0.010285366576352045<br>Epoch #574 Error:0.010227700227653567<br>Epoch #575 Error:0.010170608551857608<br>Epoch #576 Error:0.010114083560601988<br>Epoch #577 Error:0.01005811740535396<br>Epoch #578 Error:0.010002702374503777<br>Epoch #579 Error:0.009947830890527089<br>Neural Network Results:<br>1.0,0.0, actual=0.9040102333814147,ideal=1.0<br>0.0,0.0, actual=0.09892634022671229,ideal=0.0<br>0.0,1.0, actual=0.904020682439766,ideal=1.0<br>1.0,1.0, actual=0.10659032105865764,ideal=0.0</pre><br>==More Information==</p>
<ul>
<li>[<a href="http://www.heatonresearch.com/encog/mprop/compare.html">http://www.heatonresearch.com/encog/mprop/compare.html</a> Multithreaded Backpropagation]</li>
</ul>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://www.heatonresearch.com/book/neural_math_calc.html" data-id="cj5q1u8mw001disvk2cksjtm6" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      

    </footer>
  </div>
  
    

  
</article>

    </div>
</div>

  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2017 by Heaton Research, Inc. - <a href="/legal/">Legal and Copyright Info</a><br>
Jeff Heaton is a computer scientist, data scientist, and indie publisher. Heaton Research is the homepage for his projects and research.
    </div>
  </div>
</footer>

  
<script>
  var disqus_shortname = 'heatonresearch';
  
  var disqus_url = 'http://www.heatonresearch.com/book/neural_math_calc.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>








<script src="/js/script.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
